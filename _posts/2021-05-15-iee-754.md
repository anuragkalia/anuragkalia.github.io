---
layout: post
title: "Floating point numbers - The magic of IEEE-754"
date: 2021-05-15 11:00:56 +0530
---

Why are we talking about floating point humbers again? Integers are easy to think about. But floating point numbers and their binary notation is a mystery to us. Usually, we do not need to think about it. But they are inescapable in JavaScript world. All numbers are IEE-754 floating point numbers so a deeper understanding can go a long way for us.

The basic purpose of the standard is for a way to represent real numbers in memory. The standard allows two types of numbers: binary and decimal and we will only talk about binary numbers hereon.

There are multiple problems with storing a real number: 

1. A single real number can be very long, sometimes inifinitely long. So, we need to approximate it.
2. The range of real numbers is infinite and memory is finite. So some real numbers cannot be represented.
3. Real number arithmetic is not a closed system: `1/0` is Infinity and `0/0` is undefined. Hence, we need to add some extra values – `+/- Infinity`, `+/- 0`, `NaN` – to make the system closed for basic arithmetic operations.

Now, let us turn towards a notiation that helps us store a number most optimally. Scientific Notation is a way for us to represent a real number in terms of its significant digits. For example, `0.0456` can be represented as `4.56×10^(-2)` . Or,  `+4.56E-2` where base is understood to be 10. Here, `+` is the **sign**, `4.56` is called **mantissa** and `-7` is called **exponent** (always integer). Thus, instead of storing the extra 0's we can focus on storing `456` and `-7` which is more efficient. Similarly, for a binary number `0.011b` (decimal 0.375) can be written as `+1.1E-2` where base is understood to be 2. 

Though, the standard works with 16 bit numbers and above, they are to big for our examples. We will create our own 8-bit floating point system using the same principles as in the standard. Among the 8 bits, **1 bit will be for sign** (`0` for `+` and `1` for `-`), **3 bits will be for exponent** and the remaining **4 bits will be for mantissa**. There is no hard and fast rule about how many bits are given for mantissa and exponentl it depends on the use case: increasing exponent bits widens the range of real numbers whereas increasing mantissa allows to represent more real numbers within the same range. 

Now, for a 3 bit exponent, `2^3 - 2 = 6` orders of magnitude (from `-2` to `3`) can be used to represent "normal numbers" wherease 2 values are reserved to represent "sub-normal numbers", zeroes, Infinities and NaN. Similarly, a 4 bit mantissa can represent `4 + 1 = 5` significant bits for "normal numbers".

Let us first clarify what do we mean by "normal" and "sub-normal" numbers. Normal numbers all have 5 siginificant bits but "sub-normal" numbers are so small that we sacrifice significant bits in order to increase the range. Thus, arithmetic in sub-normal range is not as exact as we would want: their use will be clear next.

The smallest positive normal number is `+1.0000E-2`. the next number is `+1.0001E-2` and so on. Let us plot them and zero on a graph:


![Plotted normal numbers](/assets/images/normals.png "Plotted normal numbers")


This is deeply problematic. The difference between smallest positive and second smallest positive normal numbers in decimal is `0.015625` whereas the difference between zero and smallest positive normal numbers is `0.25`. It is `16` times the other difference! This can cause huge underflow problems. How can we fix this problem? By introducing more numbers between them: the sub-normal numbers. 

Sub-normal numbers are numbers whose exponent goes below the smallest possible value `-2` . For example, `0.0001` in scientific notation is `+1E-4`  or, in non-standard form, it is `+0.01E-2`. The first thing we notice is that the one's digit is no longer `1`. To represent all these numbers where `implied bit rule` is not followed, we use a special notation of all zeroes in the exponent (this is just representation: exponent is still the minimum `-2`). The mantissa's 4 bits are interpreted as `0.bbbb`. 

With this rule in mind, let us add the all these sub-normal numbers (orange dots) on the graph:

![Plotted sub-normal numbers](/assets/images/sub-normals.png "Plotted sub-normal numbers")

So, you see how the range between 0 and normal numbers is distributed evenly, if at the cost of the number of possible significant bits. Also, if someone was counting, there are only `15` dots where there should have been `16` (`2^4` for 4 mantissa bits). Where is the 16th sub-normal number? It is also on the graph. It is `0`! Or `+0` to be more precise. In binary representation, it is all zeroes whereas `-0` is all zeroes except the sign bit. For floating point arithmetic, they are considered one and the same.

Thus, we have now accounted for 7 of the 8 possible values of exponent; they account for all the finite real numbers in the IEEE system. Before we look at the other reserved exponent value (all 1's), let us look at the exact binary representation now:


```
[S] [EXPONENT] [MANTISSA]
[X] [  XXX   ] [  XXXX  ]
```


So, the highest priority is for sign, then exponent and finally mantissa.

Moreover, the exponent is stored in what is called biased notation. Here, add a bias of `+3` to get as follows:


| Exponent | Signed Binary For Exponent | Unsigned Binary for (Exponent + 3) |
| -------- | -------------------------- | ---------------------------------- |
| -2       | 110                        | 1 = 001                            |
| -1       | 111                        | 2 = 010                            |
| 0        | 000                        | 3 = 011                            |
| 1        | 001                        | 4 = 100                            |
| 2        | 010                        | 5 = 101                            |
| 3        | 011                        | 6 = 110                            |


In this representation, positive floating numbers can be comapred like unsigned integers which is computationally much easier. For example, `+1.1E-2` is stored as `00011000` and `+1.01E1` is stored as `01000100`. Even sub-normal numbers follow the pattern where `+0` is `00000000` and `+0.01E-2` is `00000100`. Note that, all of them are comparable as unsigned integers. Clever were the people in IEEE-754!

Now let us turn are attention towards the remaining numbers: +/- Infinity, quiet NaN (`qNaN`) and signaling NaN (`sNaN`).  all these numbers are indicated by a values of all 1's in exponent. `+Infinity` has `0` as sign and `0000` as mantissa. Similarly, `-Infinity` has `1` as sign and `0000` as mantissa. 

All other values are `NaN`. A quiet NaN has first mantissa bit as `1` and a signalling NaN as first mantissa bit as `0`. Thus, `qNaN` is `X1111XXX` and `sNaN` is `X1110YYY` (at least one `Y` is `1`). Canonical representation for `qNaN` is `01111000` and `sNaN` is  `01110100`. 

We have still not answered the bewildering question of quiet and signaling NaN! Let us first look at all the patterns in entirety:


| Binary Format | FP = 1 + w + t bits | Type       | Sub-parts       | Final FP Value | Sign Magnitude Number |
| ------------- | ------------------- | ---------- | --------------- | -------------- | --------------------- |
| 00000000      | 0 000 0000          | +0         | 0.25 * 0        | 0              | 0                     |
| 00000001      | 0 000 0001          | sub-normal | 0.25 * 0.0625   | 0.015625       | 1                     |
| 00001111      | 0 000 1111          | sub-normal | 0.25 * 0.9375   | 0.231875       | 15                    |
| 00010000      | 0 001 0000          | Normal     | 0.25 * 1        | 0.25           | 16                    |
| 00011111      | 0 001 1111          | Normal     | 0.25 * 1.9375   | 0.484375       | 31                    |
| 00100000      | 0 010 0000          | Normal     | 0.5 * 1         | 0.5            | 32                    |
| 00101111      | 0 010 1111          | Normal     | 0.5 * 1.9375    | 0.96875        | 47                    |
| 00110000      | 0 011 0000          | Normal     | 1 * 1           | 1              | 48                    |
| 00111111      | 0 011 1111          | Normal     | 1 * 1.9375      | 1.9375         | 63                    |
| 01100000      | 0 110 0000          | Normal     | 8 * 1           | 8              | 96                    |
| 01101111      | 0 110 1111          | Normal     | 8 * 1.9375      | 15.5           | 111                   |
| 01110000      | 0 111 0000          | +Infinity  | +Infinity       | +Infinity      | 112                   |
| 01110001      | 0 111 0001          | sNaN       | NaN             | NaN            | 113                   |
| 01110111      | 0 111 0111          | sNaN       | NaN             | NaN            | 119                   |
| 01111000      | 0 111 1000          | qNaN       | NaN             | NaN            | 120                   |
| 01111111      | 0 111 1111          | qNaN       | NaN             | NaN            | 127                   |
| 10000000      | 1 000 0000          | -0         | - 0.25 * 0      | -0             | 0                     |
| 10000001      | 1 000 0001          | sub-normal | - 0.25 * 0.0625 | -0.015625      | -1                    |
| 10001111      | 1 000 1111          | sub-normal | - 0.25 * 0.9375 | -0.231875      | -15                   |
| 10010000      | 1 001 0000          | Normal     | - 0.25 * 1      | -0.25          | -16                   |
| 10011111      | 1 001 1111          | Normal     | - 0.25 * 1.9375 | -0.484375      | -31                   |
| 10100000      | 1 010 0000          | Normal     | - 0.5 * 1       | -0.5           | -32                   |
| 10101111      | 1 010 1111          | Normal     | - 0.5 * 1.9375  | -0.96875       | -47                   |
| 10110000      | 1 011 0000          | Normal     | - 1 * 1         | -1             | -48                   |
| 10111111      | 1 011 1111          | Normal     | - 1 * 1.9375    | -1.9375        | -63                   |
| 11100000      | 1 110 0000          | Normal     | - 8 * 1         | -8             | -96                   |
| 11101111      | 1 110 1111          | Normal     | - 8 * 1.9375    | -15.5          | -111                  |
| 11110000      | 1 111 0000          | -Infinity  | -Infinity       | -Infinity      | -112                  |
| 11110001      | 1 111 0001          | sNaN       | NaN             | NaN            | -113                  |
| 11110111      | 1 111 0111          | sNaN       | NaN             | NaN            | -119                  |
| 11111000      | 1 111 1000          | qNaN       | NaN             | NaN            | -120                  |
| 11111111      | 1 111 1111          | qNaN       | NaN             | NaN            | -127                  |


Notice that the bit pattern of floating point numbers follows the sign magnitude integer notation perfectly! This makes comparisons very, very easy.

Now let us focus on the last section: the difference between qNaN and sNaN. They are a lot of values to waste for just to say a result is NaN. This is the part where this blog leaves with more questions than answers. A quiet NaN is the result of a faulty arithmetic operation. It does not raise exception and is only processed when encountered. In contrast, a signaling NaN occurs because of pragrammatic intervention and raises a trap. 

So what I do not know yet:

1. Why so many values for just two NaN?
2. Why have two NaN is the first place?
3. In which case are they actually triggered?
4. Are they proposed to be more useful in the next version of IEEE 754?

I hope it taught you something! 









Resources 

1. The IEEE 754-2018 standard
2. [Wikipedia](https://en.wikipedia.org/wiki/IEEE_754#Character_representation)
3. [Visual editor for binary 32 bit numbers](https://www.h-schmidt.net/FloatConverter/IEEE754.html)
4. [Lecture Notes by W. Kahan](https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF)
5. [SO: Difference between quiet NaN and signaling NaN](https://stackoverflow.com/questions/18118408/what-is-the-difference-between-quiet-nan-and-signaling-nan)
6. [Problems with NaN propagation in parallel processors](https://www.agner.org/optimize/nan_propagation.pdf)
   

